<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><!-- hexo injector head_begin start --><script src="https://cdn.jsdelivr.net/npm/echarts@5.5.1/dist/echarts.min.js"></script><!-- hexo injector head_begin end --><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Python爬虫 | 梨花先雪</title><meta name="author" content="梨花先雪"><meta name="copyright" content="梨花先雪"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="试试Python爬虫">
<meta property="og:type" content="article">
<meta property="og:title" content="Python爬虫">
<meta property="og:url" content="https://haydenzhy.github.io/post/c386cf1b">
<meta property="og:site_name" content="梨花先雪">
<meta property="og:description" content="试试Python爬虫">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/HaydenZHY/jsdelivr@main/image/avatar/avatar2.jpg">
<meta property="article:published_time" content="2025-01-20T07:54:34.000Z">
<meta property="article:modified_time" content="2025-01-20T14:18:06.551Z">
<meta property="article:author" content="梨花先雪">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/HaydenZHY/jsdelivr@main/image/avatar/avatar2.jpg"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/HaydenZHY/jsdelivr@main/image/avatar/avatar2.jpg"><link rel="canonical" href="https://haydenzhy.github.io/post/c386cf1b"><link rel="preconnect" href="//cdn.jsdelivr.net"/><meta/><meta/><meta/><meta/><meta/><meta/><meta/><meta/><meta/><meta/><meta/><meta/><meta/><meta/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 梨花先雪","link":"链接: ","source":"来源: 梨花先雪","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Python爬虫',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-20 22:18:06'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 13 || hour >= 13
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/HaydenZHY/jsdelivr@main/image/avatar/avatar2.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">54</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">75</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fa-solid fa-book"></i><span> 书单</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fa-solid fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fa-solid fa-comment-dots"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fa-solid fa-image"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/tools/"><i class="fa-fw fa-solid fa-toolbox"></i><span> 工具箱</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comment"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background: #626079"><nav id="nav"><span id="blog-info"><a href="/" title="梨花先雪"><span class="site-name">梨花先雪</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/books/"><i class="fa-fw fa-solid fa-book"></i><span> 书单</span></a></div><div class="menus_item"><a class="site-page" href="/movies/"><i class="fa-fw fa-solid fa-video"></i><span> 电影</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fa-solid fa-comment-dots"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fa-solid fa-image"></i><span> 图库</span></a></div><div class="menus_item"><a class="site-page" href="/tools/"><i class="fa-fw fa-solid fa-toolbox"></i><span> 工具箱</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comment"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python爬虫</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-20T07:54:34.000Z" title="发表于 2025-01-20 15:54:34">2025-01-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-20T14:18:06.551Z" title="更新于 2025-01-20 22:18:06">2025-01-20</time></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>前言</h1>
<p>由于专业选修大数据的结课小组作业里，有一个爬虫的分工，我自告奋勇选择这部分工作，希望以此作为动力让自己学习一下早就听说但一直没有去做的爬虫工作，这里就把学习到的部分以及作业过程里的一些经验和代码分享一下，希望能够帮助到需要爬取<strong>B站</strong>，<strong>知乎</strong>和<strong>微博</strong>数据的人们。</p>
<p>过程中参考了一些文章，以下3个链接是源代码的来源：<br>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/587307689" title="知乎爬虫">知乎爬虫</a><br>
<a target="_blank" rel="noopener" href="https://gitcode.com/gh_mirrors/bi/Bilivideoinfo" title="B站爬虫">B站爬虫</a><br>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_46235332/article/details/122682412" title="微博爬虫">微博爬虫</a></p>
<p>所有的完整源码放在我的Github仓库里<br>
<a target="_blank" rel="noopener" href="https://github.com/FunTotal/Crawler" title="爬虫完整代码Github链接">爬虫完整代码Github链接</a></p>
<h1>基础知识</h1>
<p>如果有学习过计算机网络的话，应该会对浏览器的原理有一些基本了解。（下面这些是我凭自己的认识说的，不一定保真）浏览器在访问网址时，会发送数据包，服务器接收请求后就会把网站的HTML代码发过来，最后我们看到的效果是这段HTML代码渲染后的结果，当然我们需要的信息肯定也包含正在这段HTML代码里。</p>
<p>而爬虫就是模拟浏览器，快速向需要的网页发送请求，接受HTML代码后直接从里面提取需要的信息，之后继续向下一个网页发送请求。所以实际上爬虫能爬取到的信息，从人工角度也都是手动能看到的信息。但是由于爬虫因为速度远快于真实的人类，所以会对网站造成负担，所以大部分的网站都是不支持爬虫这种行为，当然我们在爬虫的过程也会通过伪装，让网站看不出来我们是爬虫。而且爬虫的时候还要注意遵守法律，不要做违法的事情。</p>
<h2 id="Cookie">Cookie</h2>
<h3 id="Cookie介绍">Cookie介绍</h3>
<p>然后是大部分网站的爬虫都需要我们提供一个Cookie才能进行爬取，Cookie在我的理解里就是网站登录账号后的凭证，有了Cookie后对网页发送请求，在网页的视角上看就是一个登录账号的用户发送的请求。不然现在很多网站都有登录才能查看完整内容的限制，没有Cookie就爬取不了，而且某种程度上需要Cookie也起到了反爬虫的效果。</p>
<h3 id="Cookie查看">Cookie查看</h3>
<p>那么要如何查看自己的Cookie，首先在浏览器（我以EDGE浏览器为例）打开目标网站后按下F12启动开发者模式，然后选择<strong>网络</strong>一栏，并且注意<strong>禁用缓存</strong>。<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120110442096-862140735.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120110442096-862140735.png" alt=""></a><br>
此时浏览器就可以进入类似Wireshark一样的抓包模式，再次<strong>按下F5刷新网页</strong>，就能够看到浏览器捕获了很多的数据包，在筛选器一栏选中<strong>Fetch/XHR</strong>后，随便点几个包，往下拉就能够看到在请求标头里会有Cookie字段。<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120110731085-1650259284.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120110731085-1650259284.png" alt=""></a><br>
上图中马赛克掉的一大段就是知乎的Cookie（Cookie是有隐私性的，拥有别人的Cookie相当于登录别人的账号，所以Cookie需要保护好，不能在网上暴露）<br>
如果再下拉，还能看到UA字段<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120112912925-619349750.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120112912925-619349750.png" alt=""></a><br>
这个就代表着你的电脑的信息，所以在爬虫的时候，一般也会附上这个信息，来伪装的更像一个浏览器，为了装的更像一点，可以多准备几个UA，每次爬虫的时候随机选取一个，能够伪装的更好。</p>
<h1>Bilibili</h1>
<h2 id="功能">功能</h2>
<p>需要自己准备希望爬取的链接代码，可以爬取对应链接视频的标题、标签、作者、播放量、点赞投币收藏等数据，不需要Cookie。</p>
<h2 id="介绍">介绍</h2>
<p>爬取B站视频我采用的是下面仓库地址的源代码<br>
<a target="_blank" rel="noopener" href="https://gitcode.com/gh_mirrors/bi/Bilivideoinfo" title="B站爬虫">B站爬虫</a><br>
它需要你先准备好一个文件，里面是需要爬取的B站链接，之后运行代码就可以爬取这些链接的很多信息，并且爬取失败的也会记录下来哪些失败了。<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120113509501-1345055223.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120113509501-1345055223.png" alt=""></a></p>
<p>那么怎么准备待爬取链接，我的做法是去Github上找一下别人备份的B站热门榜历史数据，里面有记录每一天B站热门视频的链接，然后把那些链接提取到一个文件里，之后就可以进行爬取。</p>
<h2 id="代码">代码</h2>
<p>点击查看代码</p>
<p>**</p>
<p>highlighter- python</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">from openpyxl import Workbook</span><br><span class="line">import random</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">def write_error_log(message):</span><br><span class="line">    with open(&quot;video_errorlist.txt&quot;, &quot;a&quot;) as file:</span><br><span class="line">        file.write(message + &quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">def is_url(video_id_or_url):</span><br><span class="line">    return video_id_or_url.startswith(&quot;http&quot;) or video_id_or_url.startswith(&quot;https&quot;)</span><br><span class="line"></span><br><span class="line">def get_video_url(video_id_or_url):</span><br><span class="line">    if is_url(video_id_or_url):</span><br><span class="line">        return video_id_or_url</span><br><span class="line">    else:</span><br><span class="line">        return f&quot;https://www.bilibili.com/video/&#123;video_id_or_url&#125;&quot;</span><br><span class="line"></span><br><span class="line"># 反爬虫</span><br><span class="line">agent = [</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;,</span><br><span class="line">            &#x27;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11&#x27;,</span><br><span class="line">            &#x27;Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span><br><span class="line">            &#x27;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.72 Mobile Safari/537.36&#x27;</span><br><span class="line">        ]</span><br><span class="line">ua = random.choice(agent)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;user-agent&#x27;: ua,#UserAgent().chrome,</span><br><span class="line">    &#x27;Referer&#x27;:&#x27;https://www.bilibili.com/&#x27;,</span><br><span class="line">    &#x27;x-requested-with&#x27;: &#x27;XMLHttpRequest&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">input_file = &quot;idlist.txt&quot; #待爬取链接存在这个文件里</span><br><span class="line">output_file = &quot;output.xlsx&quot;</span><br><span class="line"></span><br><span class="line">new_wb = Workbook()</span><br><span class="line">new_ws = new_wb.active</span><br><span class="line">new_ws.append(</span><br><span class="line">    [&quot;标题&quot;, &quot;链接&quot;, &quot;up主&quot;, &quot;up主id&quot;, &quot;精确播放数&quot;, &quot;历史累计弹幕数&quot;, &quot;点赞数&quot;, &quot;投硬币枚数&quot;, &quot;收藏人数&quot;, &quot;转发人数&quot;,</span><br><span class="line">     &quot;发布时间&quot;, &quot;视频时长(秒)&quot;, &quot;视频简介&quot;, &quot;作者简介&quot;, &quot;标签&quot;, &quot;视频aid&quot;])</span><br><span class="line"></span><br><span class="line">with open(input_file, &quot;r&quot;) as file:</span><br><span class="line">    id_list = file.readlines()</span><br><span class="line"></span><br><span class="line">i = 0</span><br><span class="line">for video_id_or_url in id_list:</span><br><span class="line">    i += 1</span><br><span class="line">    url = get_video_url(video_id_or_url.strip())</span><br><span class="line">    try:</span><br><span class="line">        # time.sleep(1)</span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        soup = BeautifulSoup(response.text, &quot;html.parser&quot;)</span><br><span class="line"></span><br><span class="line">        # 视频 aid、视频时长和作者 id</span><br><span class="line">        initial_state_script = soup.find(&quot;script&quot;, text=re.compile(&quot;window.__INITIAL_STATE__&quot;))</span><br><span class="line">        initial_state_text = initial_state_script.string</span><br><span class="line"></span><br><span class="line">        author_id_pattern = re.compile(r&#x27;&quot;mid&quot;:(\d+)&#x27;)</span><br><span class="line">        video_aid_pattern = re.compile(r&#x27;&quot;aid&quot;:(\d+)&#x27;)</span><br><span class="line">        video_duration_pattern = re.compile(r&#x27;&quot;duration&quot;:(\d+)&#x27;)</span><br><span class="line"></span><br><span class="line">        author_id = author_id_pattern.search(initial_state_text).group(1)</span><br><span class="line">        video_aid = video_aid_pattern.search(initial_state_text).group(1)</span><br><span class="line">        video_duration_raw = int(video_duration_pattern.search(initial_state_text).group(1))</span><br><span class="line">        video_duration = video_duration_raw - 2</span><br><span class="line"></span><br><span class="line">        # 提取标题</span><br><span class="line">        title_raw = soup.find(&quot;title&quot;).text</span><br><span class="line">        title = re.sub(r&quot;_哔哩哔哩_bilibili&quot;, &quot;&quot;, title_raw).strip()</span><br><span class="line"></span><br><span class="line">        # 提取标签</span><br><span class="line">        keywords_content = soup.find(&quot;meta&quot;, itemprop=&quot;keywords&quot;)[&quot;content&quot;]</span><br><span class="line">        content_without_title = keywords_content.replace(title + &#x27;,&#x27;, &#x27;&#x27;)</span><br><span class="line">        keywords_list = content_without_title.split(&#x27;,&#x27;)</span><br><span class="line">        tags = &quot;,&quot;.join(keywords_list[:-4])</span><br><span class="line"></span><br><span class="line">        meta_description = soup.find(&quot;meta&quot;, itemprop=&quot;description&quot;)[&quot;content&quot;]</span><br><span class="line">        numbers = re.findall(</span><br><span class="line">            r&#x27;[\s\S]*?视频播放量 (\d+)、弹幕量 (\d+)、点赞数 (\d+)、投硬币枚数 (\d+)、收藏人数 (\d+)、转发人数 (\d+)&#x27;,</span><br><span class="line">            meta_description)</span><br><span class="line"></span><br><span class="line">        # 提取作者</span><br><span class="line">        author_search = re.search(r&quot;视频作者\s*([^,]+)&quot;, meta_description)</span><br><span class="line">        if author_search:</span><br><span class="line">            author = author_search.group(1).strip()</span><br><span class="line">        else:</span><br><span class="line">            author = &quot;未找到作者&quot;</span><br><span class="line"></span><br><span class="line">        # 提取作者简介</span><br><span class="line">        author_desc_pattern = re.compile(r&#x27;作者简介 (.+?),&#x27;)</span><br><span class="line">        author_desc_match = author_desc_pattern.search(meta_description)</span><br><span class="line">        if author_desc_match:</span><br><span class="line">            author_desc = author_desc_match.group(1)</span><br><span class="line">        else:</span><br><span class="line">            author_desc = &quot;未找到作者简介&quot;</span><br><span class="line"></span><br><span class="line">        # 提取视频简介</span><br><span class="line">        meta_parts = re.split(r&#x27;,\s*&#x27;, meta_description)</span><br><span class="line">        if meta_parts:</span><br><span class="line">            video_desc = meta_parts[0].strip()</span><br><span class="line">        else:</span><br><span class="line">            video_desc = &quot;未找到视频简介&quot;</span><br><span class="line"></span><br><span class="line">        if numbers:</span><br><span class="line">            views, danmaku, likes, coins, favorites, shares = [int(n) for n in numbers[0]]</span><br><span class="line">            publish_date = soup.find(&quot;meta&quot;, itemprop=&quot;uploadDate&quot;)[&quot;content&quot;]</span><br><span class="line">            new_ws.append([title, url, author, author_id, views, danmaku, likes, coins, favorites, shares, publish_date, video_duration, video_desc, author_desc, tags, video_aid])</span><br><span class="line">            print(f&quot;第&#123;i&#125;行视频&#123;url&#125;已完成爬取&quot;)</span><br><span class="line">        else:</span><br><span class="line">            print(f&quot;第&#123;i&#125;行视频 &#123;url&#125;未找到相关数据，可能为分集视频&quot;)</span><br><span class="line"></span><br><span class="line">    except Exception as e:</span><br><span class="line">        write_error_log(f&quot;第&#123;i&#125;行视频发生错误：&#123;e&#125;&quot;)</span><br><span class="line">        print(f&quot;第&#123;i&#125;行发生错误，已记录到错误日志:出错数据为&#123;video_id_or_url&#125;&quot;)</span><br><span class="line"></span><br><span class="line">new_wb.save(output_file)</span><br></pre></td></tr></table></figure>
<h1>知乎</h1>
<h2 id="功能-2">功能</h2>
<p>爬取一个问题下的回答内容和回答的创建时间</p>
<h2 id="介绍-2">介绍</h2>
<p>知乎的爬取是依据以下链接文章里的教程<br>
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/587307689" title="知乎爬虫">知乎爬虫</a><br>
大致思路是发现知乎的回答里会有一个包，指向下一个回答的id，就类似链表一样，从一个初始的链接开始，不断地把下一个回答的id存起来，就把问题下的回答的id都记录下来，之后再根据这些id爬取对应的回答内容。<br>
具体的，分为两个文件，第一个crawler.py就是记录answer_id，第二个crawler2.py是根据answer_id爬取内容，下面会介绍一下具体怎么修改代码来爬取需要的内容。</p>
<h2 id="需要修改的内容">需要修改的内容</h2>
<p>首先是crawler.py里的template<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120115908028-348043365.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120115908028-348043365.png" alt=""></a></p>
<p>需要在待爬取问题，类似找Cookie一样，找到下面这个包 feeds?..这个包，里面的请求URL替换掉crawler.py里的template。<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120114629184-286111761.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120114629184-286111761.png" alt=""></a></p>
<p>记得网址里面的offset=后面要加一个{offset}，就是模仿之前写在那里的格式<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120120257903-1894029061.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120120257903-1894029061.png" alt=""></a></p>
<p>然后是代码里的Cookie替换为自己知乎的Cookie，贴在单引号里面<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120120403580-3517804.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120120403580-3517804.png" alt=""></a></p>
<p>最后把代码里headers里的refer也换一下（这个好像不换也行，但我是换了），refer字段就在Cookie下面一点<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120120550647-1354037006.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120120550647-1354037006.png" alt=""></a></p>
<p>**</p>
<p>highlighter- 1c</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">headers = &#123;</span><br><span class="line">    &#x27;cookie&#x27; : cookie,</span><br><span class="line">    &#x27;user-agent&#x27;: ua,#UserAgent().chrome,</span><br><span class="line">    &#x27;Referer&#x27;:&#x27;https://www.zhihu.com/question/654859896/answer/3487198664&#x27;,</span><br><span class="line">    &#x27;x-requested-with&#x27;: &#x27;XMLHttpRequest&#x27;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>之后就可以爬取，对于保存信息方面，实测知乎的反爬有点强，爬个两百多面就提示说对方主机强制断开连接，所以可以在代码设置一下每爬取若干页就保存一次。</p>
<p>**</p>
<p>highlighter- reasonml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 每隔100条保存一次信息</span><br><span class="line">if (page % 100 == 0):</span><br><span class="line">    df = pd.DataFrame(&#123;&#x27;answer_id&#x27;: answer_ids&#125;)  # 重新创建df</span><br><span class="line">    df.to_csv(&#x27;answer_id.csv&#x27;, index=True)</span><br></pre></td></tr></table></figure>
<p>然后就可以使用crawler2.py来爬取这些answerid对应的回答，需要在crawler2.py里修改下面红线标出的部分，Cookie和refer和前面类似，下面那个url那里记得把问题的id换成对应的你要爬取的那个问题的id，就是网页地址里面能明显看出来。<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120121047473-648090074.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120121047473-648090074.png" alt=""></a></p>
<p>之后运行就可以爬取了，同样记得每隔一段保存一次，代码里是直接保存在不同的文件里，我通过clean.py进行整合和去重。具体的应该看看clean.py就知道怎么改了，只要修改一下文件路径基本就ok。</p>
<h2 id="完整代码">完整代码</h2>
<h3 id="crawler-py"><a target="_blank" rel="noopener" href="http://crawler.py">crawler.py</a></h3>
<p>点击查看代码</p>
<p>**</p>
<p>highlighter- llvm</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import pandas as pd</span><br><span class="line">import time</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">template = &#x27;https://www.zhihu.com/api/v4/questions/656533798/feeds?include=data%5B*%5D.is_normal%2Cadmin_closed_comment%2Creward_info%2Cis_collapsed%2Cannotation_action%2Cannotation_detail%2Ccollapse_reason%2Cis_sticky%2Ccollapsed_by%2Csuggest_edit%2Ccomment_count%2Ccan_comment%2Ccontent%2Ceditable_content%2Cattachment%2Cvoteup_count%2Creshipment_settings%2Ccomment_permission%2Ccreated_time%2Cupdated_time%2Creview_info%2Crelevant_info%2Cquestion%2Cexcerpt%2Cis_labeled%2Cpaid_info%2Cpaid_info_content%2Creaction_instruction%2Crelationship.is_authorized%2Cis_author%2Cvoting%2Cis_thanked%2Cis_nothelp%3Bdata%5B*%5D.author.follower_count%2Cvip_info%2Ckvip_info%2Cbadge%5B*%5D.topics%3Bdata%5B*%5D.settings.table_of_content.enabled&amp;offset=&#123;offset&#125;&amp;limit=3&amp;order=default&amp;ws_qiangzhisafe=1&amp;platform=desktop&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df = pd.DataFrame()</span><br><span class="line"># df有三列，answer_id和content以及创建日期</span><br><span class="line">df[&#x27;answer_id&#x27;] = []</span><br><span class="line">df[&#x27;content&#x27;] = []</span><br><span class="line">df[&#x27;created_time&#x27;] = []</span><br><span class="line"></span><br><span class="line">answer_ids = []</span><br><span class="line"></span><br><span class="line">agent = [</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;,</span><br><span class="line">            &#x27;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11&#x27;,</span><br><span class="line">            &#x27;Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span><br><span class="line">            &#x27;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.72 Mobile Safari/537.36&#x27;</span><br><span class="line">        ]</span><br><span class="line">ua = random.choice(agent)</span><br><span class="line"></span><br><span class="line">cookies = [</span><br><span class="line">    &#x27;&#x27;</span><br><span class="line">]</span><br><span class="line">cookie = random.choice(cookies)</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;cookie&#x27; : cookie,</span><br><span class="line">    &#x27;user-agent&#x27;: ua,#UserAgent().chrome,</span><br><span class="line">    &#x27;Referer&#x27;:&#x27;https://www.zhihu.com/question/654859896/answer/3487198664&#x27;,</span><br><span class="line">    &#x27;x-requested-with&#x27;: &#x27;XMLHttpRequest&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 第一条使用模版，后面的都是next来获取</span><br><span class="line">url0 = template.format(offset=0)</span><br><span class="line">resp0 = requests.get(url0, headers=headers)</span><br><span class="line">for data in resp0.json()[&#x27;data&#x27;]:</span><br><span class="line">        answer_id = data[&#x27;target&#x27;][&#x27;id&#x27;]</span><br><span class="line">        answer_ids.append(answer_id)</span><br><span class="line">next = resp0.json()[&#x27;paging&#x27;][&#x27;next&#x27;]</span><br><span class="line"></span><br><span class="line">for page in range(1,5001):# 这里自己估算一下，每页是5条数据</span><br><span class="line">    #对第page页进行访问</span><br><span class="line">    headers1 = &#123;</span><br><span class="line">    &#x27;cookie&#x27; : random.choice(cookies),</span><br><span class="line">    &#x27;user-agent&#x27;: random.choice(agent),#UserAgent().chrome,</span><br><span class="line">    &#x27;Referer&#x27;:&#x27;https://www.zhihu.com/question/654859896/answer/3487198664&#x27;,</span><br><span class="line">    &#x27;x-requested-with&#x27;: &#x27;XMLHttpRequest&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">    resp = requests.get(next, headers=headers1)</span><br><span class="line">    print(&#x27;正在爬取第&#x27; + str(page) + &#x27;页&#x27;)</span><br><span class="line">    </span><br><span class="line">    for data in resp.json()[&#x27;data&#x27;]:</span><br><span class="line">        answer_id = data[&#x27;target&#x27;][&#x27;id&#x27;]</span><br><span class="line">        # 添加answer_id到df中</span><br><span class="line">        answer_ids.append(answer_id)</span><br><span class="line">    next = resp.json()[&#x27;paging&#x27;][&#x27;next&#x27;]</span><br><span class="line">    time.sleep(random.randint(1,4))</span><br><span class="line">    # 每隔100条保存一次信息</span><br><span class="line">    if (page % 100 == 0):</span><br><span class="line">        df = pd.DataFrame(&#123;&#x27;answer_id&#x27;: answer_ids&#125;)  # 重新创建df</span><br><span class="line">        df.to_csv(&#x27;answer_id.csv&#x27;, index=True)</span><br></pre></td></tr></table></figure>
<h3 id="crawler2-py"><a target="_blank" rel="noopener" href="http://crawler2.py">crawler2.py</a></h3>
<p>点击查看代码</p>
<p>**</p>
<p>highlighter- python</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import pandas as pd</span><br><span class="line">import random</span><br><span class="line">import requests</span><br><span class="line">import pandas as pd</span><br><span class="line">import time</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line">contents = []</span><br><span class="line">created_times = []</span><br><span class="line">agent = [</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;,</span><br><span class="line">            &#x27;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11&#x27;,</span><br><span class="line">            &#x27;Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span><br><span class="line">            &#x27;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.72 Mobile Safari/537.36&#x27;</span><br><span class="line">        ]</span><br><span class="line">ua = random.choice(agent)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;cookie&#x27; : &#x27;&#x27;,</span><br><span class="line">    &#x27;user-agent&#x27;: ua,#UserAgent().chrome,</span><br><span class="line">    &#x27;Referer&#x27;:&#x27;https://www.zhihu.com/question/654859896/answer/3487198664&#x27;,</span><br><span class="line">    &#x27;x-requested-with&#x27;: &#x27;XMLHttpRequest&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 读取answer_id.csv文件</span><br><span class="line">df = pd.read_csv(&#x27;answer_id.csv&#x27;)</span><br><span class="line"></span><br><span class="line"># 提取answer_id列</span><br><span class="line">answer_ids = df[&#x27;answer_id&#x27;].tolist()</span><br><span class="line">cookies = [</span><br><span class="line">    &#x27;&#x27;</span><br><span class="line">]</span><br><span class="line">batch = 0</span><br><span class="line">for answer_id in answer_ids:</span><br><span class="line">    print(&#x27;正在爬取answer_id为&#123;answer_id&#125;的数据&#x27;.format(answer_id=answer_id))</span><br><span class="line">    url = &#x27;https://www.zhihu.com/question/654859896/answer/&#123;answer_id&#125;&#x27;.format(answer_id=answer_id)</span><br><span class="line">    try:</span><br><span class="line">        headers1 = &#123;</span><br><span class="line">        &#x27;cookie&#x27; : random.choice(cookies),</span><br><span class="line">        &#x27;user-agent&#x27;: random.choice(agent),#UserAgent().chrome,</span><br><span class="line">        &#x27;Referer&#x27;:&#x27;https://www.zhihu.com/question/654859896/answer/3487198664&#x27;,</span><br><span class="line">        &#x27;x-requested-with&#x27;: &#x27;XMLHttpRequest&#x27;</span><br><span class="line">        &#125;</span><br><span class="line">        resp = requests.get(url, headers=headers1)</span><br><span class="line">        soup = BeautifulSoup(resp.text, &#x27;html.parser&#x27;)</span><br><span class="line">        # 查找content</span><br><span class="line">        content = soup.find(&#x27;div&#x27;, class_=&#x27;RichContent-inner&#x27;).text</span><br><span class="line">        contents.append(content)</span><br><span class="line">        time_element = soup.find(&#x27;span&#x27;, &#123;&#x27;data-tooltip&#x27;: True&#125;)</span><br><span class="line">        if time_element:</span><br><span class="line">            created_time = time_element[&#x27;data-tooltip&#x27;].replace(&#x27;发布于 &#x27;, &#x27;&#x27;)</span><br><span class="line">            created_times.append(created_time)</span><br><span class="line">        else:</span><br><span class="line">            created_times.append(&#x27;-&#x27;)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        print(f&#x27;爬取answer_id为&#123;answer_id&#125;的数据时出现异常：&#123;e&#125;&#x27;)</span><br><span class="line">        break</span><br><span class="line">    </span><br><span class="line">    time.sleep(random.randint(1,2))</span><br><span class="line"></span><br><span class="line">    # 每爬取500个回答就保存一次数据,保存在不同的文件中</span><br><span class="line">    if len(contents) % 500 == 0:</span><br><span class="line">        new_data = &#123;&#x27;answer_id&#x27;: answer_ids[:len(contents)], &#x27;content&#x27;: contents, &#x27;created_time&#x27; : created_times[:len(contents)]&#125;</span><br><span class="line">        new_df = pd.DataFrame(new_data)</span><br><span class="line">        new_df.to_csv(f&#x27;text_&#123;batch&#125;.csv&#x27;, index=True)</span><br><span class="line">        batch += 1</span><br></pre></td></tr></table></figure>
<h1>微博</h1>
<h2 id="功能-3">功能</h2>
<p>对于一个话题，爬取话题链接,楼主ID,话题内容,楼主昵称,楼主性别,是否认证,认证类型,是否认证金v,发博数量,关注人数,粉丝数,微博等级,发布日期,发布时间,转发量,评论量,点赞量。这些信息。</p>
<h2 id="介绍-3">介绍</h2>
<p>微博的爬虫代码是来源于以下链接，链接里面有爬取评论的内容，但是我作业里不需要，我就把这个功能删了，需要爬取评论的可以去下面链接看看。<br>
<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_46235332/article/details/122682412" title="微博爬虫">微博爬虫</a></p>
<h2 id="具体操作">具体操作</h2>
<p>微博爬虫比较麻烦，好像是有限制一次性最多爬取50面，所以只能手动通过筛选时间，爬50面后看最后一条消息的时间，把筛选时间截止到那里，再接着爬50面，每次都得手动调，很麻烦。</p>
<p>需要修改的内容如下：<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120122007874-1159636649.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120122007874-1159636649.png" alt=""></a></p>
<p>里面的Cookie和Referer就不用说了，老生常谈，主要说一下最底下那个baseurl，就是在微博里面高级搜索，筛选时间后，得到的网址。<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120122145488-1694643046.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120122145488-1694643046.png" alt=""></a><br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120122206095-1956403865.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120122206095-1956403865.png" alt=""></a><br>
上面的网址就是baseurl，替换后直接运行就可以开爬了，需要注意的是，代码里有一段是写csv的表头的语句，因为前面说了得多次爬取，所以只有第一次爬取的时候需要写表头，后面就得把写表头的语句注释掉。<br>
<a href="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120122359649-689719132.png"><img src="./Python%E7%88%AC%E8%99%AB%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93_Bilibili%EF%BC%8C%E7%9F%A5%E4%B9%8E%EF%BC%8C%E5%BE%AE%E5%8D%9A%20-%20TJUHuangTao%20-%20%E5%8D%9A%E5%AE%A2%E5%9B%AD_files/3483649-20250120122359649-689719132.png" alt=""></a></p>
<h2 id="完整代码-2">完整代码</h2>
<p>点击查看代码</p>
<p>**</p>
<p>highlighter- python</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"> </span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">微博爬虫，爬取一个主题下的博文内容，博主信息，评论等各种信息</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"> </span><br><span class="line">import requests, random, re</span><br><span class="line">import time</span><br><span class="line">import os</span><br><span class="line">import csv</span><br><span class="line">import sys</span><br><span class="line">import json</span><br><span class="line">import importlib</span><br><span class="line"># from fake_useragent import UserAgent</span><br><span class="line">from lxml import etree</span><br><span class="line">import datetime</span><br><span class="line">import pandas as pd</span><br><span class="line">from selenium import webdriver</span><br><span class="line">import urllib.request</span><br><span class="line"> </span><br><span class="line"># 记录起始时间</span><br><span class="line">importlib.reload(sys)</span><br><span class="line">startTime = time.time()</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">设置文件储存的路径 </span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">path1=&#x27;./weibo_content_gaokao.csv&#x27;#存取的是微博博文的信息（不包含评论）</span><br><span class="line">csvfile1 = open(path1, &#x27;a&#x27;, newline=&#x27;&#x27;, encoding=&#x27;utf-8-sig&#x27;) #&#x27;a&#x27;是追加模式, &#x27;w&#x27;是重写</span><br><span class="line">writer_1=csv.writer(csvfile1)</span><br><span class="line"></span><br><span class="line"># csv头部</span><br><span class="line">writer_1.writerow((&#x27;话题链接&#x27;,  &#x27;楼主ID&#x27;, &#x27;话题内容&#x27;,&#x27;楼主昵称&#x27;, &#x27;楼主性别&#x27;,&#x27;是否认证&#x27;,&#x27;认证类型&#x27;,</span><br><span class="line">                   &#x27;是否认证金v&#x27;,&#x27;发博数量&#x27;,&#x27;关注人数&#x27;,&#x27;粉丝数&#x27;,&#x27;微博等级&#x27;, &#x27;发布日期&#x27;,</span><br><span class="line">                   &#x27;发布时间&#x27;, &#x27;转发量&#x27;, &#x27;评论量&#x27;, &#x27;点赞量&#x27;))        #微博博文的信息（不包含评论）</span><br><span class="line"></span><br><span class="line"># --------------------------------------------头部信息-----------------------------------------------------</span><br><span class="line">ip_list = [</span><br><span class="line">            &#123;&#x27;http&#x27;: &#x27;http://118.193.47.193:8118&#x27;&#125;, # 湖南长沙</span><br><span class="line">            &#123;&#x27;http&#x27;: &#x27;http://58.20.234.243:9091&#x27;&#125;, # 湖南湘潭</span><br><span class="line">            &#123;&#x27;http&#x27;: &#x27;http://58.20.235.180:9091&#x27;&#125;, # 湖南湘潭</span><br><span class="line">            &#123;&quot;http&quot;: &quot;http://112.115.57.20:3128&quot;&#125;,</span><br><span class="line">            &#123;&#x27;http&#x27;: &#x27;http://121.41.171.223:3128&#x27;&#125;,</span><br><span class="line">            &#123;&quot;http&quot;: &quot;http://124.88.67.54:80&quot;&#125;,</span><br><span class="line">            &#123;&quot;http&quot;: &quot;http://61.135.217.7:80&quot;&#125;,</span><br><span class="line">            &#123;&quot;http&quot;: &quot;http://42.231.165.132:8118&quot;&#125;,</span><br><span class="line">            &#123;&quot;http&quot;: &quot;http://10.10.1.10:3128&quot;&#125;,</span><br><span class="line">            &#123;&quot;https&quot;: &quot;http://10.10.1.10:1080&quot;&#125;</span><br><span class="line">        ]</span><br><span class="line">ip = random.choice(ip_list)</span><br><span class="line"></span><br><span class="line"># 反爬虫</span><br><span class="line">agent = [</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;,</span><br><span class="line">            &#x27;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; en) Presto/2.8.131 Version/11.11&#x27;,</span><br><span class="line">            &#x27;Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;,</span><br><span class="line">            &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; TencentTraveler 4.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; The World)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)&#x27;,</span><br><span class="line">            &#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#x27;</span><br><span class="line">            &#x27;Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.72 Mobile Safari/537.36&#x27;</span><br><span class="line">        ]</span><br><span class="line">ua = random.choice(agent)</span><br><span class="line"></span><br><span class="line">cookie_list = [</span><br><span class="line">    # 手机号</span><br><span class="line">    &#123;&#x27;cookie&#x27;:&#x27;&#x27;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">cookie = random.choice(cookie_list)[&#x27;cookie&#x27;]</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;cookie&#x27;: cookie,</span><br><span class="line">    &#x27;user-agent&#x27;: ua,#UserAgent().chrome,</span><br><span class="line">    &#x27;Referer&#x27;:&#x27;https://m.weibo.cn/search?containerid=100103type%3D1%26q%3D%E6%98%A5%E8%8A%82&#x27;,</span><br><span class="line">    &#x27;x-requested-with&#x27;: &#x27;XMLHttpRequest&#x27;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"># # -----------------------------------爬取该主题首页的每个主题的ID------------------------------------------</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">找出发布者id，并存入列表，用于找每个具体博客的网址</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">comments_ID = []</span><br><span class="line">baseurl = &#x27;https://s.weibo.com/weibo?q=%23%E9%AB%98%E8%80%83%23&amp;xsort=hot&amp;suball=1&amp;timescope=custom%3A2015-01-01%3A2016-05-12-20&amp;Refer=g&#x27;</span><br><span class="line">def get_title_id():</span><br><span class="line">    for page in range(1, 51):  # 每个页面大约有9个话题</span><br><span class="line">        headers = &#123;</span><br><span class="line">            &#x27;cookie&#x27;: cookie,</span><br><span class="line">            &#x27;user-agent&#x27;: ua,</span><br><span class="line">            &#x27;Referer&#x27;: f&#x27;&#123;baseurl&#125;&amp;page=&#123;page&#125;&#x27;,</span><br><span class="line">            &#x27;x-requested-with&#x27;: &#x27;XMLHttpRequest&#x27;</span><br><span class="line">        &#125;</span><br><span class="line">        time.sleep(1)</span><br><span class="line">        api_url = f&quot;&#123;baseurl&#125;&amp;page=&#123;page&#125;&quot;</span><br><span class="line"> </span><br><span class="line">        rep1 = requests.get(url=api_url, headers=headers)</span><br><span class="line">        try:</span><br><span class="line">            rep=rep1.text # 获取ID值并写入列表comment_ID中</span><br><span class="line">            comment_ID=re.findall(&#x27;(?&lt;=mid=&quot;)\d&#123;16&#125;&#x27;, rep)</span><br><span class="line">            comments_ID.extend(comment_ID)</span><br><span class="line">            print(page,&quot;页id获取成功！&quot;,comment_ID)</span><br><span class="line">        except:</span><br><span class="line">            print(page,&quot;页id获取有误！&quot;)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"># -----------------------------------爬取该主题下每个博客的详情页面 ------------------------------------------</span><br><span class="line"> </span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">该主题下每个博客主的详情（包括话题内容、楼主id、楼主昵称、楼主性别、发布时间、日期、</span><br><span class="line">发布时间、转发量、评论量、点赞量）</span><br><span class="line">（利用正则表达式抓取）</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">is_continue=&#x27;y&#x27;</span><br><span class="line">start_date = pd.to_datetime(&#x27;2015/01/01&#x27;)</span><br><span class="line">end_date = pd.to_datetime(&#x27;2024/12/31&#x27;)</span><br><span class="line">def spider_title(comment_ID):</span><br><span class="line"></span><br><span class="line">    article_url = &#x27;https://m.weibo.cn/detail/&#x27; + comment_ID</span><br><span class="line">    print(&quot;article_url = &quot;, article_url)</span><br><span class="line">    time.sleep(1)</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        html_text = requests.get(url=article_url, headers=headers).text</span><br><span class="line">        # 发布日期</span><br><span class="line">        created_title_time = re.findall(&#x27;.*?&quot;created_at&quot;: &quot;(.*?)&quot;.*?&#x27;, html_text)[0].split(&#x27; &#x27;)</span><br><span class="line">        # print(created_title_time)</span><br><span class="line">        # 日期</span><br><span class="line">        if &#x27;Jan&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;01&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Feb&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;02&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Mar&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;03&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Apr&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;04&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;May&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;05&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Jun&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;06&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;July&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;07&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Aug&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;08&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Sep&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;09&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Oct&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;10&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Nov&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;11&#x27;, created_title_time[2])</span><br><span class="line">        elif &#x27;Dec&#x27; in created_title_time:</span><br><span class="line">            title_created_YMD = &quot;&#123;&#125;/&#123;&#125;/&#123;&#125;&quot;.format(created_title_time[-1], &#x27;12&#x27;, created_title_time[2])</span><br><span class="line">        # print(&quot;title_created_YMD = &quot;, title_created_YMD)</span><br><span class="line"> </span><br><span class="line">        print(&#x27;发布日期：&#x27;,title_created_YMD)</span><br><span class="line">        time2 = pd.to_datetime(title_created_YMD)</span><br><span class="line"> </span><br><span class="line">        if start_date&lt;= time2 &lt;= end_date:</span><br><span class="line">            # 话题内容</span><br><span class="line">            find_title = re.findall(&#x27;.*?&quot;text&quot;: &quot;(.*?)&quot;,.*?&#x27;, html_text)[0]</span><br><span class="line">            title_text = re.sub(&#x27;&lt;(S*?)[^&gt;]*&gt;.*?|&lt;.*? /&gt;&#x27;, &#x27;&#x27;, find_title)  # 正则匹配掉html标签</span><br><span class="line"></span><br><span class="line">            # 楼主ID</span><br><span class="line">            title_user_id = re.findall(&#x27;.*?&quot;id&quot;: (.*?),.*?&#x27;, html_text)[1]</span><br><span class="line"></span><br><span class="line">            # 楼主昵称</span><br><span class="line">            title_user_NicName = re.findall(&#x27;.*?&quot;screen_name&quot;: &quot;(.*?)&quot;,.*?&#x27;, html_text)[0]</span><br><span class="line"></span><br><span class="line">            # 楼主性别</span><br><span class="line">            title_user_gender = re.findall(&#x27;.*?&quot;gender&quot;: &quot;(.*?)&quot;,.*?&#x27;, html_text)[0]</span><br><span class="line"></span><br><span class="line">            verified=re.findall(&#x27;.*?&quot;verified&quot;: (.*?),.*?&#x27;, html_text)[0]#楼主是否认证</span><br><span class="line">            if verified==&#x27;true&#x27;:</span><br><span class="line">                verified_type_ext = re.findall(&#x27;.*?&quot;verified_type_ext&quot;: (.*?),.*?&#x27;, html_text)[0] # 楼主是否金v</span><br><span class="line">            else:</span><br><span class="line">                verified_type_ext=0</span><br><span class="line">            # print(verified_type_ext)</span><br><span class="line">            content_num=re.findall(&#x27;.*?&quot;statuses_count&quot;: (.*?),.*?&#x27;, html_text)[0] #楼主发博数量</span><br><span class="line">            verified_type=re.findall(&#x27;.*?&quot;verified_type&quot;: (.*?),.*?&#x27;, html_text)[0]#楼主认证类型</span><br><span class="line">            urank=re.findall(&#x27;.*?&quot;urank&quot;: (.*?),.*?&#x27;, html_text)[0]#楼主微博等级</span><br><span class="line">            guanzhu=re.findall(&#x27;.*?&quot;follow_count&quot;: (.*?),.*?&#x27;, html_text)[0]#楼主关注数</span><br><span class="line">            fensi=eval(re.findall(&#x27;.*?&quot;followers_count&quot;: (.*?),.*?&#x27;, html_text)[0])#楼主粉丝数</span><br><span class="line"></span><br><span class="line">            # 发布时间</span><br><span class="line">            add_title_time = created_title_time[3]</span><br><span class="line">            print(&quot;add_title_time = &quot;, add_title_time)</span><br><span class="line">            #当该条微博是是转发微博时，会有一个原微博的转发评论点赞量，以及本条微博的转发评论点赞量，此时需要的是第2个元素</span><br><span class="line">            if len(re.findall(&#x27;.*?&quot;reposts_count&quot;: (.*?),.*?&#x27;, html_text))&gt;1:</span><br><span class="line">                # 转发量</span><br><span class="line">                reposts_count = re.findall(&#x27;.*?&quot;reposts_count&quot;: (.*?),.*?&#x27;, html_text)[1]</span><br><span class="line">                # 评论量</span><br><span class="line">                comments_count = re.findall(&#x27;.*?&quot;comments_count&quot;: (.*?),.*?&#x27;, html_text)[1]</span><br><span class="line">                print(&quot;comments_count = &quot;, comments_count)</span><br><span class="line">                # 点赞量</span><br><span class="line">                attitudes_count = re.findall(&#x27;.*?&quot;attitudes_count&quot;: (.*?),.*?&#x27;, html_text)[1]</span><br><span class="line">                # 每个ajax一次加载20条数据</span><br><span class="line">                comment_count = int(int(comments_count) / 20)</span><br><span class="line">            else:</span><br><span class="line">                # 转发量</span><br><span class="line">                reposts_count = re.findall(&#x27;.*?&quot;reposts_count&quot;: (.*?),.*?&#x27;, html_text)[0]</span><br><span class="line">                # print(&quot;reposts_count = &quot;, reposts_count)</span><br><span class="line"></span><br><span class="line">                # 评论量</span><br><span class="line">                comments_count = re.findall(&#x27;.*?&quot;comments_count&quot;: (.*?),.*?&#x27;, html_text)[0]</span><br><span class="line">                print(&quot;comments_count = &quot;, comments_count)</span><br><span class="line"></span><br><span class="line">                # 点赞量</span><br><span class="line">                attitudes_count = re.findall(&#x27;.*?&quot;attitudes_count&quot;: (.*?),.*?&#x27;, html_text)[0]</span><br><span class="line">                # print(&quot;attitudes_count = &quot;, attitudes_count)</span><br><span class="line"></span><br><span class="line">                # 每个ajax一次加载20条数据</span><br><span class="line">                comment_count = int(int(comments_count) / 20)</span><br><span class="line"></span><br><span class="line">            # position1是记录</span><br><span class="line">            position11 = (article_url, title_user_id, title_text, title_user_NicName, title_user_gender, verified, verified_type,</span><br><span class="line">            verified_type_ext, content_num, guanzhu, fensi, urank, title_created_YMD, add_title_time,reposts_count, comments_count, attitudes_count)</span><br><span class="line"></span><br><span class="line">            # 写入数据</span><br><span class="line">            writer_1.writerow(position11)</span><br><span class="line">            print(&#x27;写入博文信息数据成功！&#x27;)</span><br><span class="line">            return comment_count, title_user_id, title_created_YMD,title_text</span><br><span class="line"></span><br><span class="line">            global is_continue</span><br><span class="line">        else:</span><br><span class="line">            is_continue = input(&#x27;日期超出范围,是否继续爬取博文信息?(y/n, 默认: y) ——&gt; &#x27;)#输入是否继续爬取</span><br><span class="line">            if is_continue == &#x27;y&#x27; or is_continue == &#x27;yes&#x27; or not is_continue:</span><br><span class="line">                pass</span><br><span class="line">            else:</span><br><span class="line">                print(&#x27;日期超出范围，停止爬取博文信息！&#x27;)</span><br><span class="line">                # 计算使用时间</span><br><span class="line">                endTime = time.time()</span><br><span class="line">                useTime = (endTime - startTime) / 60</span><br><span class="line">                print(&quot;该次所获的信息一共使用%s分钟&quot; % useTime)</span><br><span class="line">                sys.exit(0)</span><br><span class="line">            return is_continue</span><br><span class="line">    except:</span><br><span class="line">        print(&#x27;博文网页解析错误，或微博不存在或暂无查看权限！&#x27;)</span><br><span class="line">        pass</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># -------------------------------------------------主函数---------------------------------------------------</span><br><span class="line">def main():</span><br><span class="line">    count_title = len(comments_ID)</span><br><span class="line">    for count, comment_ID in enumerate(comments_ID):</span><br><span class="line">        print(&quot;正在爬取第%s条微博，一共找到个%s条微博需要爬取&quot; % (count + 1, count_title))</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            maxPage,title_user_id,title_created_YMD,title_text = spider_title(comment_ID)</span><br><span class="line">        except:</span><br><span class="line">            if is_continue == &#x27;y&#x27; or is_continue == &#x27;yes&#x27; or not is_continue:</span><br><span class="line">                print(&quot;--------------------------分隔符---------------------------&quot;)</span><br><span class="line">                pass</span><br><span class="line">            else:</span><br><span class="line">                sys.exit(0)</span><br><span class="line">        print(&quot;--------------------------分隔符---------------------------&quot;)</span><br><span class="line">    csvfile1.close()</span><br><span class="line">    </span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    # 获取话题ID</span><br><span class="line">    get_title_id()</span><br><span class="line">    # 主函数操作</span><br><span class="line">    main()</span><br><span class="line">    # 计算使用时间</span><br><span class="line">    endTime = time.time()</span><br><span class="line">    useTime = (endTime - startTime) / 60</span><br><span class="line">    print(&quot;该次所获的信息一共使用%s分钟&quot; % useTime)</span><br><span class="line">    # print(&#x27;错误页面:&#x27;,error_page_list)</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://haydenzhy.github.io/">梨花先雪</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://haydenzhy.github.io/post/c386cf1b">https://haydenzhy.github.io/post/c386cf1b</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://HaydenZHY.github.io" target="_blank">梨花先雪</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/HaydenZHY/jsdelivr@main/image/avatar/avatar2.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/e4bd7c43" title="24251寒假"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">24251寒假</div></div></a></div><div class="next-post pull-right"><a href="/post/72b5a919" title="脚本"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">脚本</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/post/6e3f7550" title="VSCode中运行Python程序"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-23</div><div class="title">VSCode中运行Python程序</div></div></a></div><div><a href="/post/298117e1" title="数值计算方法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-08</div><div class="title">数值计算方法</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/HaydenZHY/jsdelivr@main/image/avatar/avatar2.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">梨花先雪</div><div class="author-info__description">言必信，行必果</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">54</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">75</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HaydenZHY"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/HaydenZHY" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:243038409@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my Blog !</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Cookie"><span class="toc-number">2.1.</span> <span class="toc-text">Cookie</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Cookie%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.1.</span> <span class="toc-text">Cookie介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Cookie%E6%9F%A5%E7%9C%8B"><span class="toc-number">2.1.2.</span> <span class="toc-text">Cookie查看</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">Bilibili</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD"><span class="toc-number">3.1.</span> <span class="toc-text">功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.2.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">3.3.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">知乎</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD-2"><span class="toc-number">4.1.</span> <span class="toc-text">功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D-2"><span class="toc-number">4.2.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9C%80%E8%A6%81%E4%BF%AE%E6%94%B9%E7%9A%84%E5%86%85%E5%AE%B9"><span class="toc-number">4.3.</span> <span class="toc-text">需要修改的内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-number">4.4.</span> <span class="toc-text">完整代码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#crawler-py"><span class="toc-number">4.4.1.</span> <span class="toc-text">crawler.py</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#crawler2-py"><span class="toc-number">4.4.2.</span> <span class="toc-text">crawler2.py</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">微博</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD-3"><span class="toc-number">5.1.</span> <span class="toc-text">功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D-3"><span class="toc-number">5.2.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C"><span class="toc-number">5.3.</span> <span class="toc-text">具体操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-2"><span class="toc-number">5.4.</span> <span class="toc-text">完整代码</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/45705220" title="利用PyTorch实现基于LSTM+CRF的命名实体识别模型">利用PyTorch实现基于LSTM+CRF的命名实体识别模型</a><time datetime="2025-03-21T03:54:00.000Z" title="发表于 2025-03-21 11:54:00">2025-03-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/4311997a" title="《智能搜索与问答》实验记录">《智能搜索与问答》实验记录</a><time datetime="2025-03-14T03:11:38.000Z" title="发表于 2025-03-14 11:11:38">2025-03-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/d1bf3f24" title="《计算机视觉技术》实验记录">《计算机视觉技术》实验记录</a><time datetime="2025-03-14T03:10:50.000Z" title="发表于 2025-03-14 11:10:50">2025-03-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/3e062841" title="从零构建大模型">从零构建大模型</a><time datetime="2025-03-08T14:20:47.000Z" title="发表于 2025-03-08 22:20:47">2025-03-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/post/289f3202" title="Windows---CMD常用指令">Windows---CMD常用指令</a><time datetime="2025-03-01T14:55:18.000Z" title="发表于 2025-03-01 22:55:18">2025-03-01</time></div></div></div></div></div></div></main><footer id="footer" style="background: #626079"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By 梨花先雪</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="/js/tw_cn.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(()=>{
  const getGiscusTheme = theme => {
    return theme === 'dark' ? 'dark' : 'light'
  }

  const loadGiscus = () => {
    const config = Object.assign({
      src: 'https://giscus.app/client.js',
      'data-repo': 'HaydenZHY/HaydenZHY.github.io',
      'data-repo-id': 'R_kgDOMtPJmQ',
      'data-category-id': 'DIC_kwDOMtPJmc4ClrGo',
      'data-mapping': 'pathname',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true
    },null)

    const ele = document.createElement('script')
    for (let key in config) {
      ele.setAttribute(key, config[key])
    }
    document.getElementById('giscus-wrap').appendChild(ele)
  }

  const changeGiscusTheme = theme => {
    const sendMessage = message => {
      const iframe = document.querySelector('iframe.giscus-frame')
      if (!iframe) return
      iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app')
    }

    sendMessage({
      setConfig: {
        theme: getGiscusTheme(theme)
      }
    });
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment= loadGiscus
  }
})()</script></div><script id="canvas_nest" defer="defer" color="121,206,226" opacity="0.7" zIndex="-1" count="88" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="true"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["link[rel=\"canonical\"]","meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"]):not([href="/movies/"]):not([href="/books/"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener
  btf.removeGlobalFnEvent('pjax')
  btf.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404')
  }
})</script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = 'a996012aef93459d8c1f716106a6a684';
  var gaud_map_key = 'd85e2aaaa9c353eeacde11351c3f7484';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '112.982279,28.19409';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><!-- hexo injector body_end end --></body></html>